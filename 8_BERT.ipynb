{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a855f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import cross_entropy,softmax, relu\n",
    "\n",
    "import utils\n",
    "#from GPT import GPT\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0b789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e7a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT = __import__('7_GPT').GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1614baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764a477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_RATE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88fbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(GPT):\n",
    "\n",
    "    def __init__(\n",
    "        self, model_dim, max_len, num_layer, num_head, n_vocab, lr,\n",
    "        max_seg=3, drop_rate=0.2, padding_idx=0) -> None:\n",
    "        super().__init__(model_dim, max_len, num_layer, num_head, n_vocab, lr, max_seg, drop_rate, padding_idx)\n",
    "    \n",
    "    def step(self,seqs,segs,seqs_, loss_mask,nsp_labels):\n",
    "        device = next(self.parameters()).device\n",
    "        self.opt.zero_grad()\n",
    "        mlm_logits, nsp_logits = self(seqs, segs, training=True)    # [n, step, n_vocab], [n, n_cls]\n",
    "        mlm_loss = cross_entropy(\n",
    "            torch.masked_select(mlm_logits,loss_mask).reshape(-1,mlm_logits.shape[2]),\n",
    "            torch.masked_select(seqs_,loss_mask.squeeze(2))\n",
    "            )\n",
    "        nsp_loss = cross_entropy(nsp_logits,nsp_labels.reshape(-1))\n",
    "        loss = mlm_loss + 0.2 * nsp_loss\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        return loss.cpu().data.numpy(),mlm_logits\n",
    "\n",
    "    def mask(self, seqs):\n",
    "        mask = torch.eq(seqs,self.padding_idx)\n",
    "        return mask[:, None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057e40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_loss_mask(len_arange, seq, pad_id):\n",
    "    rand_id = np.random.choice(len_arange, size=max(2, int(MASK_RATE * len(len_arange))), replace=False)\n",
    "    loss_mask = np.full_like(seq, pad_id, dtype=np.bool)\n",
    "    loss_mask[rand_id] = True\n",
    "    return loss_mask[None, :], rand_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8532c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mask(seq, len_arange, pad_id, mask_id):\n",
    "    loss_mask, rand_id = _get_loss_mask(len_arange, seq, pad_id)\n",
    "    seq[rand_id] = mask_id\n",
    "    return loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22462810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_replace(seq, len_arange, pad_id, word_ids):\n",
    "    loss_mask, rand_id = _get_loss_mask(len_arange, seq, pad_id)\n",
    "    seq[rand_id] = torch.from_numpy(np.random.choice(word_ids, size=len(rand_id))).type(torch.IntTensor)\n",
    "    return loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a0a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(seq, len_arange, pad_id):\n",
    "    loss_mask, _ = _get_loss_mask(len_arange, seq, pad_id)\n",
    "    return loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55a03c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask_or_replace(data,arange,dataset):\n",
    "    seqs, segs,xlen,nsp_labels = data\n",
    "    seqs_ = seqs.data.clone()\n",
    "    p = np.random.random()\n",
    "    if p < 0.7:\n",
    "        # mask\n",
    "        loss_mask = np.concatenate([\n",
    "            do_mask(\n",
    "                seqs[i],\n",
    "                np.concatenate((arange[:xlen[i,0]],arange[xlen[i,0]+1:xlen[i].sum()+1])),\n",
    "                dataset.pad_id,\n",
    "                dataset.mask_id\n",
    "                )\n",
    "                for i in range(len(seqs))], axis=0)\n",
    "    elif p < 0.85:\n",
    "        # do nothing\n",
    "        loss_mask = np.concatenate([\n",
    "            do_nothing(\n",
    "                seqs[i],\n",
    "                np.concatenate((arange[:xlen[i,0]],arange[xlen[i,0]+1:xlen[i].sum()+1])),\n",
    "                dataset.pad_id\n",
    "                )\n",
    "                for i in range(len(seqs))],  axis=0)\n",
    "    else:\n",
    "        # replace\n",
    "        loss_mask = np.concatenate([\n",
    "            do_replace(\n",
    "                seqs[i],\n",
    "                np.concatenate((arange[:xlen[i,0]],arange[xlen[i,0]+1:xlen[i].sum()+1])),\n",
    "                dataset.pad_id,\n",
    "                dataset.word_ids\n",
    "                )\n",
    "                for i in range(len(seqs))],  axis=0)\n",
    "    loss_mask = torch.from_numpy(loss_mask).unsqueeze(2)\n",
    "    return seqs, segs, seqs_, loss_mask, xlen, nsp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8728ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    MODEL_DIM = 256\n",
    "    N_LAYER = 4\n",
    "    LEARNING_RATE = 1e-4\n",
    "    dataset = utils.MRPCData(\"./MRPC\",2000)\n",
    "    print(\"num word: \",dataset.num_word)\n",
    "    model = BERT(\n",
    "        model_dim=MODEL_DIM, max_len=dataset.max_len, num_layer=N_LAYER, num_head=4, n_vocab=dataset.num_word,\n",
    "        lr=LEARNING_RATE, max_seg=dataset.num_seg, drop_rate=0.2, padding_idx=dataset.pad_id\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU train avaliable\")\n",
    "        device =torch.device(\"cuda\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = model.cpu()\n",
    "    \n",
    "    loader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "    arange = np.arange(0,dataset.max_len)\n",
    "    for epoch in range(500):\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            seqs, segs, seqs_, loss_mask, xlen, nsp_labels = random_mask_or_replace(batch,arange,dataset)\n",
    "            seqs, segs, seqs_, nsp_labels, loss_mask = seqs.type(torch.LongTensor).to(device), segs.type(torch.LongTensor).to(device),seqs_.type(torch.LongTensor).to(device),nsp_labels.to(device),loss_mask.to(device)\n",
    "            loss, pred = model.step(seqs, segs, seqs_, loss_mask, nsp_labels)\n",
    "            if batch_idx % 100 == 0:\n",
    "                pred = pred[0].cpu().data.numpy().argmax(axis=1)\n",
    "                print(\n",
    "                \"\\n\\nEpoch: \",epoch,\n",
    "                \"|batch: \", batch_idx,\n",
    "                \"| loss: %.3f\" % loss,\n",
    "                \"\\n| tgt: \", \" \".join([dataset.i2v[i] for i in seqs[0].cpu().data.numpy()[:xlen[0].sum()+1]]),\n",
    "                \"\\n| prd: \", \" \".join([dataset.i2v[i] for i in pred[:xlen[0].sum()+1]]),\n",
    "                \"\\n| tgt word: \", [dataset.i2v[i] for i in (seqs_[0]*loss_mask[0].view(-1)).cpu().data.numpy() if i != dataset.v2i[\"<PAD>\"]],\n",
    "                \"\\n| prd word: \", [dataset.i2v[i] for i in pred*(loss_mask[0].view(-1).cpu().data.numpy()) if i != dataset.v2i[\"<PAD>\"]],\n",
    "                )\n",
    "#     os.makedirs(\"./visual/models/bert\",exist_ok=True)\n",
    "#     torch.save(model.state_dict(),\"./visual/models/bert/model.pth\")\n",
    "#     export_attention(model,device,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b68454cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_attention(model,device,data,name=\"bert\"):\n",
    "    model.load_state_dict(torch.load(\"./visual/models/bert/model.pth\",map_location=device))\n",
    "    seqs, segs,xlen,nsp_labels = data[:32]\n",
    "    seqs, segs,xlen,nsp_labels = torch.from_numpy(seqs),torch.from_numpy(segs),torch.from_numpy(xlen),torch.from_numpy(nsp_labels)\n",
    "    seqs, segs,nsp_labels = seqs.type(torch.LongTensor).to(device), segs.type(torch.LongTensor).to(device),nsp_labels.to(device)\n",
    "    model(seqs,segs,False)\n",
    "    seqs = seqs.cpu().data.numpy()\n",
    "    data = {\"src\": [[data.i2v[i] for i in seqs[j]] for j in range(len(seqs))], \"attentions\": model.attentions}\n",
    "    path = \"./visual/tmp/%s_attention_matrix.pkl\" % name\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aebea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:All]",
   "language": "python",
   "name": "conda-env-All-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
